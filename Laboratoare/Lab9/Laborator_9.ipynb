{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs-pub-ro/ML/blob/master/lab/lab9/Laborator_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmaHAc_KHG_5"
   },
   "source": [
    "# Rețele neurale pentru clasificare imaginilor\n",
    "\n",
    "_Tudor Berariu, 2018_ (tudor.berariu@gmail.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKCKTWvsuJuF"
   },
   "source": [
    "În cadrul acestui laborator veți implementa o rețea neurală cu arhitectură convoluțională pentru clasificarea imaginilor.\n",
    "\n",
    "Rețeaua va fi compusă din straturi convoluționale, straturi de Pooling pentru reducerea dimensionalității și activări de tip ReLU.\n",
    "Rețeaua va avea straturi lineare la final și un strat softmax înainte de ieșiri. \n",
    "\n",
    "Funcția de cost folosită este negative log likelihood. Pentru optimizarea acesteia se va folosi SGD (stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Resurse teoretice\n",
    "\n",
    "* Citiți de aici despre straturile convoluțional și max pooling:\n",
    "http://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "* Citiți de aici despre cum se implementează eficient pașii de convoluție folosind funcția im2col din numpy:\n",
    "https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgNIWhzoHOIz"
   },
   "source": [
    "## 1. Setul de date MNIST\n",
    "\n",
    "Setul de date MNIST este compus din imagini de 28x28 pixeli reprezentând una dintre cele zece cifre 0-9.\n",
    "\n",
    "Decomentați mai jos comanda `!pip install mnist` pentru a instala pachetul `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQiqJyO7E7Ek"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: mnist in /home/teo/.local/lib/python3.8/site-packages (0.2.2)\nRequirement already satisfied: numpy in /home/teo/.local/lib/python3.8/site-packages (from mnist) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mnist\n",
    "\n",
    "import mnist\n",
    "train_imgs = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_imgs = mnist.test_images()\n",
    "test_labels  = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElGfqnPzuJuO"
   },
   "source": [
    "### Exemple din setul de date MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeftJ_CpE7Eu"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avJh9wQquJuU"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Labels: [7 1 3 7 8 9 1 7 5 3 6 7 6 6 1]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"56.798772pt\" version=\"1.1\" viewBox=\"0 0 368.925 56.798772\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-05-11T19:52:23.713065</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 56.798772 \nL 368.925 56.798772 \nL 368.925 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 32.920647 \nL 361.725 32.920647 \nL 361.725 10.600647 \nL 26.925 10.600647 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa10e323c1b)\">\n    <image height=\"23\" id=\"image7dfdd80b45\" transform=\"scale(1 -1)translate(0 -23)\" width=\"335\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-9.920647\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mdecf50df71\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.323571\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(24.142321 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.180714\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(60.818214 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.037857\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(97.494107 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.895\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(137.35125 47.519085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.752143\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(177.208393 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"226.609286\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(217.065536 47.519085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"266.466429\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g transform=\"translate(256.922679 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.323571\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 350 -->\n      <g transform=\"translate(296.779821 47.519085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.180714\" xlink:href=\"#mdecf50df71\" y=\"32.920647\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 400 -->\n      <g transform=\"translate(336.636964 47.519085)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2431584769\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2431584769\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m2431584769\" y=\"30.92779\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 34.727009)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 32.920647 \nL 26.925 10.600647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 361.725 32.920647 \nL 361.725 10.600647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 32.920647 \nL 361.725 32.920647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 10.600647 \nL 361.725 10.600647 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa10e323c1b\">\n   <rect height=\"22.32\" width=\"334.8\" x=\"26.925\" y=\"10.600647\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "idxs = np.random.randint(0, len(train_imgs), 15)\n",
    "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
    "plt.imshow(imgs)\n",
    "print(\"Labels:\", train_labels[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFP9QnUJuJuY"
   },
   "source": [
    "### Standardizarea datelor\n",
    "\n",
    "Datele de intrare (imaginile) vor fi rescalate pentru a avea media zero și deviația standard 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWmt4XVxuJuZ"
   },
   "outputs": [],
   "source": [
    "mean, std  = train_imgs.mean(), train_imgs.std()\n",
    "train_imgs = (train_imgs - mean) / std\n",
    "test_imgs = (test_imgs - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgERUA07IuSr"
   },
   "source": [
    "## 2. Construirea unei rețele de tip feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BE-V6sONuJud"
   },
   "source": [
    "### Notații\n",
    "  - dimensiunea datelor de intrare este $D = 28 * 28 = 784$, iar dimensiunea ieșirilor rețelei este $K=10$ (numărul de clase)\n",
    "  - rețeaua neurală va avea $L$ straturi\n",
    "  - $B$ va reprezenta dimensiunea batch-ului (numărul de exemple trecute în același timp prin rețea)\n",
    "  - Vom nota cu ${\\bf X} \\in {\\mathbb R}^{B \\times D}$ un batch de intrări $\\left\\lbrace {\\bf x}_0, {\\bf x}_1, \\dots {\\bf x}_B \\right\\rbrace$ și similar ${\\bf Y} \\in {\\mathbb R}^{B \\times K}$\n",
    "  - ${\\bf x}^{(l)}$ reprezintă intrările stratului $l$ (${\\bf x}^{(0)}$ va fi o imagine precum cele din setul MNIST de dimensiune $D$)\n",
    "  - ${\\bf y}^{(l)}$ reprezintă ieșirile stratului $l$ (${\\bf y}^{(L-1)}$ reprezintă ieșirile rețelei)\n",
    "  - ${\\bf \\theta}^{(l)}$ reprezintă parametrii stratului $l$\n",
    "  - ${\\cal L}$ reprezintă funcția de cost ( _negative log likelihood_ )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YTu4tS8uJue"
   },
   "source": [
    "### Straturile rețelei\n",
    "\n",
    "Unele straturi au parametri ce trebuie optimizați în timpul antrenării. Vom nota parametrii stratului $l$ cu $\\bf{\\theta}^{(l)}$.\n",
    "Fiecare strat pe care îl veți implementa va avea trei metode:\n",
    " - `forward` calculează și întoarce ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf \\theta}^{(l)}\\right)$\n",
    " - `backward` primește $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, reține intern $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}^{(l)}}$ și întoarce $\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l)}}$\n",
    " - `update` modifică parametrii locali ${\\bf \\theta}^{(l)}$ folosing gradientul stocat $\\frac{\\partial{\\cal L}}{\\partial{\\bf \\theta}^{(l)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SW206j3euJuf"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, *args, **kwargs):\n",
    "        pass  # If a layer has no parameters, then this function does nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIT6K4IduJuk"
   },
   "source": [
    "### Rețeaua neurală\n",
    "\n",
    "  * în faza `forward` ieșirile stratului $l$ devin intrările stratului $l+1$: ${\\bf x}^{(l+1)} = {\\bf y}^{(l)}$\n",
    "  * în faza `backward` gradientul în raport cu intrările stratului $l+1$ devine gradientul în raport cu ieșirile stratului $l$: $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}=\\frac{\\partial {\\cal L}}{\\partial {\\bf x}^{(l+1)}}$\n",
    "  \n",
    "Completați metoda `backward` din clasa `FeedForwardNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTn-g3KAuJul"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "    \n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
    "        self._inputs = []\n",
    "        for layer in self.layers:\n",
    "            if train:\n",
    "                self._inputs.append(x)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
    "        # TODO <0> : Compute the backward phase\n",
    "        dx = dy\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dx = self.layers[i].backward(self._inputs[i], dx)\n",
    "\n",
    "        del self._inputs\n",
    "        return dx\n",
    "    \n",
    "    def update(self, *args, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            layer.update(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VyQLVM4quJup"
   },
   "source": [
    "### Stratul linear\n",
    "\n",
    "Un strat linear cu $M$ intrări și $N$ ieșiri are parametrii $\\theta = \\left( {\\bf W}, {\\bf b} \\right)$ unde ${\\bf W} \\in \\mathbb{R}^{M \\times N}$ și ${\\bf b} \\in \\mathbb{R}^{N}$.\n",
    "\n",
    "Pentru un singur exemlu ${\\bf x} \\in {\\mathbb R}^{M}$:\n",
    "$$ {\\bf y} = {\\bf x}^{\\intercal}{\\bf W} + {\\bf b} $$\n",
    "\n",
    "Implementați metoda `forward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și întoarce ieșirile corespunzătoare: $Y \\in {\\mathbb R}^{B\\times N}$.\n",
    "\n",
    "Implementați metoda `backward` care primește un batch de exemple $X \\in {\\mathbb R}^{B\\times M}$ și gradientul în raport cu ieșirile $\\frac{\\partial {\\cal L}}{\\partial {\\bf Y}}$ și realizează două lucruri:\n",
    "  - calculează și salvează intern gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf \\theta}}$\n",
    "  - calculează și întoarce gradientul $\\frac{\\partial {\\cal L}}{\\partial {\\bf X}}$\n",
    "  \n",
    "Implementați strategia de optimizare SGD cu _momentum_ (în metoda `update`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S47ZsyKdE7FF"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, insize: int, outsize: int) -> None:\n",
    "        bound = np.sqrt(6. / insize)\n",
    "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
    "        self.bias = np.zeros((outsize,))\n",
    "        \n",
    "        self.dweight = np.zeros_like(self.weight)\n",
    "        self.dbias = np.zeros_like(self.bias)\n",
    "\n",
    "        self.wv = 0\n",
    "        self.bv = 0\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # compute the output of a linear layer\n",
    "        return x @ self.weight + self.bias\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # compute dweight, dbias and  return dx\n",
    "        self.dweight = x.T @ dy\n",
    "        self.dbias = np.sum(dy, axis=0)\n",
    "\n",
    "        return dy @ self.weight.T\n",
    "    \n",
    "    def update(self, mode='SGD', lr=0.001, mu=0.9):\n",
    "        if mode == 'SGD':\n",
    "            self.weight -= lr * self.dweight\n",
    "            self.bias -= lr * self.dbias\n",
    "        elif mode == 'momentum':\n",
    "            # implement momentum update\n",
    "            self.wv = mu * self.wv + lr * self.dweight\n",
    "            self.weight -= self.wv\n",
    "\n",
    "            self.bv = mu * self.bv + lr * self.dbias\n",
    "            self.bias -= self.bv\n",
    "        else:\n",
    "            raise ValueError('mode should be SGD or momentum, not ' + str(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgfHlVgDuJut"
   },
   "source": [
    "### The Rectified Linear Unit\n",
    "\n",
    "Stratul ReLU aplică următoare următoare transformare neliniară element cu element:\n",
    "$$y = \\max\\left(x, 0\\right)$$\n",
    "\n",
    "Implementați metodele `forward` și `backward` pentru un strat de activare ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOR1DJiwE7FJ"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # TODO <3> : Compute the output of a rectified linear unit\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
    "        # TODO <4> : Compute the gradient w.r.t. x\n",
    "        return dy * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Convolutional Layer\n",
    "\n",
    "**[Cerințele 9.1 - 9.2] (4p)** Implementați metodele `forward` și `backward` pentru un strat convolutional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Im2Col auxiliries from [CS231n assignment](https://github.com/huyouare/CS231n/blob/master/assignment2/cs231n/im2col.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "  # First figure out what the size of the output should be\n",
    "  N, C, H, W = x_shape\n",
    "  assert (H + 2 * padding - field_height) % stride == 0\n",
    "  assert (W + 2 * padding - field_height) % stride == 0\n",
    "  out_height = (H + 2 * padding - field_height) // stride + 1\n",
    "  out_width = (W + 2 * padding - field_width) // stride + 1\n",
    "\n",
    "  i0 = np.repeat(np.arange(field_height), field_width)\n",
    "  i0 = np.tile(i0, C)\n",
    "  i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "  j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "  j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "  i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "  j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "  k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "  return (k, i, j)\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "  \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "  # Zero-pad the input\n",
    "  p = padding\n",
    "  x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "  k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "\n",
    "  cols = x_padded[:, k, i, j]\n",
    "  C = x.shape[1]\n",
    "  cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "  return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "  \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "  N, C, H, W = x_shape\n",
    "  H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "  x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "  k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "  cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "  cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "  np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "  if padding == 0:\n",
    "    return x_padded\n",
    "  return x_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<>:34: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n<>:55: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n<>:34: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n<>:55: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n<ipython-input-10-4bd462edcaa4>:34: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert (dd, hh, ww == self.inputs_depth, self.inputs_height, self.inputs_width)\n<ipython-input-10-4bd462edcaa4>:55: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert (output_errors.shape == batch_size, self.outputs_depth, self.outputs_height, self.outputs_width)\n"
     ]
    }
   ],
   "source": [
    "class ConvolutionalLayer(Layer):\n",
    "    def __init__(self, inputs_depth: int, inputs_height: int, inputs_width: int, outputs_depth: int, k: int, stride: int):\n",
    "        # Number of inputs, number of outputs, filter size, stride\n",
    "\n",
    "        self.inputs_depth = inputs_depth\n",
    "        self.inputs_height = inputs_height\n",
    "        self.inputs_width = inputs_width\n",
    "\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "\n",
    "        self.outputs_depth = outputs_depth\n",
    "        self.outputs_height = int((self.inputs_height - self.k) / self.stride + 1)\n",
    "        self.outputs_width = int((self.inputs_width - self.k) / self.stride + 1)\n",
    "\n",
    "        # Layer's parameters\n",
    "        self.weights = np.random.normal(\n",
    "            0,\n",
    "            np.sqrt(2.0 / float(self.outputs_depth + self.inputs_depth + self.k + self.k)),\n",
    "            (self.outputs_depth, self.inputs_depth, self.k, self.k)\n",
    "        )\n",
    "        self.biases = np.random.normal(\n",
    "            0,\n",
    "            np.sqrt(2.0 / float(self.outputs_depth + 1)),\n",
    "            (self.outputs_depth, 1)\n",
    "        )\n",
    "\n",
    "        # Gradients\n",
    "        self.g_weights = np.zeros(self.weights.shape)\n",
    "        self.g_biases = np.zeros(self.biases.shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "        assert (dd, hh, ww == self.inputs_depth, self.inputs_height, self.inputs_width)\n",
    "\n",
    "        # Computed values\n",
    "        outputs = np.zeros((batch_size, self.outputs_depth, self.outputs_height, self.outputs_width))\n",
    "\n",
    "        # TODO (9.1)\n",
    "        # -> compute outputs\n",
    "        inputs_col = im2col_indices(inputs, self.weights.shape[2], self.weights.shape[3], 0, self.stride)\n",
    "        w_col = self.weights.reshape((self.weights.shape[0], -1))\n",
    "        b_col = self.biases.reshape(-1, 1)\n",
    "        outputs = w_col @ inputs_col + b_col\n",
    "\n",
    "        outputs = outputs.reshape(self.outputs_depth, self.outputs_height, self.outputs_width, batch_size)\n",
    "\n",
    "        outputs = outputs.transpose(3, 0, 1, 2)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def backward(self, inputs: np.ndarray, output_errors: np.ndarray) -> np.ndarray:\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "        assert (output_errors.shape == batch_size, self.outputs_depth, self.outputs_height, self.outputs_width)\n",
    "\n",
    "        # TODO (9.2.i)\n",
    "        # Compute the gradients w.r.t. the bias terms (self.g_biases)\n",
    "        self.g_biases = np.sum(output_errors, axis=(0, 2, 3)).reshape((self.weights.shape[0], -1))\n",
    "        \n",
    "        # TODO (9.2.ii)\n",
    "        # Compute the gradients w.r.t. the weights (self.g_weights)\n",
    "        inputs_col = im2col_indices(inputs, self.weights.shape[2], self.weights.shape[3], 0, self.stride)\n",
    "        dout_reshaped = output_errors.transpose(1, 2, 3, 0).reshape(self.weights.shape[0], -1)\n",
    "        self.g_weights = (dout_reshaped @ inputs_col.T).reshape(self.weights.shape)\n",
    "\n",
    "        # TODO (9.2.iii)\n",
    "        # Compute and return the gradients w.r.t the inputs of this layer\n",
    "        # return these gradients\n",
    "        dx_cols = self.weights.reshape(self.weights.shape[0], -1).T @ dout_reshaped\n",
    "        dx = col2im_indices(dx_cols, inputs.shape, self.k, self.k, 0, self.stride)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "    def update_parameters(self, mode='SGD', lr=0.001, mu=0.9):\n",
    "        if mode == 'SGD':\n",
    "            self.weights -= lr * self.g_weights\n",
    "            self.biases -= lr * self.g_biases\n",
    "        elif mode == 'momentum':\n",
    "            # implement/reuse momentum update from Lab 8\n",
    "            self.wv = mu * self.wv + lr * self.dweight\n",
    "            self.weight -= self.wv\n",
    "\n",
    "            self.bv = mu * self.bv + lr * self.dbias\n",
    "            self.bias -= self.bv\n",
    "        else:\n",
    "            raise ValueError('mode should be SGD or momentum, not ' + str(mode))\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[C ((%s, %s, %s) -> (%s, %s ) -> (%s, %s, %s)]\" % (\n",
    "            self.inputs_depth, self.inputs_height, self.inputs_width, self.k, self.stride, self.outputs_depth,\n",
    "            self.outputs_height, self.outputs_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Max Pooling Layer\n",
    "\n",
    "**[Cerința 9.3] (2p)** Implementați metodele `forward` și `backward` pentru un strat de tip Max Pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer(Layer):\n",
    "\n",
    "    def __init__(self, size: int = 2, stride: int = 2):\n",
    "        # Dimensions: stride\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "        # indexes of max activations\n",
    "        self.switches = {}\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "\n",
    "        h_out = (hh - self.size) // self.stride + 1\n",
    "        w_out = (ww - self.size) // self.stride + 1\n",
    "\n",
    "        in_col = im2col_indices(\n",
    "            inputs.reshape(batch_size * dd, 1, hh, ww),\n",
    "            self.size, self.size, 0, self.stride)\n",
    "\n",
    "        self.switches = np.argmax(in_col, 0)\n",
    "\n",
    "        return in_col[self.switches, range(self.switches.size)]\\\n",
    "            .reshape(h_out, w_out, batch_size, dd)\\\n",
    "            .transpose(2, 3, 0, 1)\n",
    "\n",
    "\n",
    "    def backward(self, inputs, output_errors):\n",
    "        batch_size, dd, hh, ww = inputs.shape\n",
    "\n",
    "        in_col = im2col_indices(\n",
    "            inputs.reshape(batch_size * dd, 1, hh, ww),\n",
    "            self.size, self.size, 0, self.stride)\n",
    "        din_col = np.zeros_like(in_col)\n",
    "\n",
    "        dout = output_errors.transpose(2, 3, 0, 1).ravel()\n",
    "        din_col[self.switches, range(self.switches.size)] = dout\n",
    "\n",
    "        return col2im_indices(\n",
    "            din_col,\n",
    "            (batch_size * dd, 1, hh, ww),\n",
    "            self.size, self.size, 0, self.stride)\\\n",
    "                .reshape(inputs.shape)\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[MP (%s x %s)]\" % (self.size, self.stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linearization Layer\n",
    "\n",
    "**[Cerința 9.4] (2p)** Implementați metodele `forward` și `backward` pentru un strat de tip Linearization.\n",
    "\n",
    "**Nota** Acesta este un strat care rearanjeaza datele dintr-un volum (numar canale x inaltime x latime) intr-un singur vector; (util la trecerea de la volume de imagini la straturi complet conectate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearizeLayer(Layer):\n",
    "\n",
    "    def __init__(self, depth: int, height: int, width: int):\n",
    "        # Dimensions: depth, height, width\n",
    "        self.depth = depth\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # inputs are of shape (batch_size, depth, height, width)\n",
    "        # outputs are of shape (batch_size, depth x height x width)\n",
    "        \n",
    "        # TODO 1\n",
    "        # Reshape inputs- transform volume to column\n",
    "        return inputs.reshape(inputs.shape[0], -1)\n",
    "\n",
    "\n",
    "    def backward(self, inputs: np.ndarray, output_errors: np.ndarray) -> np.ndarray:\n",
    "        # unused argument - inputs\n",
    "        # output_errors of shape (batch_size, depth x height x width)\n",
    "\n",
    "        # TODO 1\n",
    "        # Reshape gradients - transform column to volume\n",
    "        return output_errors.reshape(-1, self.depth, self.height, self.width)\n",
    "\n",
    "\n",
    "    def to_string(self):\n",
    "        return \"[Lin ((%s, %s, %s) -> %s)]\" % (self.depth, self.height, self.width, self.depth * self.height * self.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NrWBTmbI9gW"
   },
   "source": [
    "## 3. Funcția de cost\n",
    "\n",
    "Funcția de cost pe care o vom folosi este _cross entropy_ care combină un _softmax_ și un cost _negative log-likelihood_. (Matematica la tablă)\n",
    "\n",
    "Dacă ${\\bf y}$ reprezintă ieșrile rețelei pentru o intrare ${\\bf x}$, atunci ${\\bf y}$ va avea o dimensiune egală cu numărul de clase $K$. Atunci probabilitatea (prezisă de rețea) ca exemplul ${\\bf x}$ să aparțină clasei $k$ va fi $p_k$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_k &= \\frac{e^{y_k}}{\\sum_j e^{y_j}} & & \\text{softmax} \\\\\n",
    "{\\cal L} &= -\\log p_t & & \\text{negative log-likelihood}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Pentru un batch de dimensiune $B$ se va face media costurilor corespunzătare fiecărui exemplu ($p_k$ este o funcție de ${\\bf x}$ și ${\\bf \\theta}$):\n",
    "\n",
    "$$ {\\cal L} = \\frac{1}{B} \\sum_{({\\bf x}, {\\bf t}) \\in Batch} -\\log p_t \\left({\\bf x}, \\theta\\right) $$\n",
    "\n",
    "Implementați metodele `forward` și `backward` pentru un funcția de cost _cross-entropy_ (o vom privi ca pe un strat suplimentar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDXiDEu8E7FW"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _softmax(self, y, t):\n",
    "        e_t = np.exp([y[i][t[i]] for i in range(len(t))])\n",
    "        return e_t / np.exp(y).sum(axis=1)\n",
    "\n",
    "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
    "        # TODO <5> : Compute the negative log likelihood\n",
    "        return -np.mean(np.log(self._softmax(y, t)))\n",
    "    \n",
    "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
    "        # TODO <6> : Compute dl/dy\n",
    "        diff = [[yj == y[i][t[i]] for yj in y[i]] for i in range(y.shape[0])]\n",
    "\n",
    "        return [[(np.exp(y[i][j]) / np.sum(np.exp(y[i])) - diff[i][j]) / y.shape[0]\\\n",
    "            for j in range(len(y[i]))] for i in range(len(y))]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uz9qM5eHJLNw"
   },
   "source": [
    "### Acuratețea\n",
    "\n",
    "Calculați acuratețea predicțiilor ${\\bf y}$ în raport cu clasele corecte ${\\bf t}$ (rația exemplelor pentru care clasa corectă a avut probabilitatea prezisă maximă)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nYfVCBSE7Fe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
    "    return accuracy_score(t, np.argmax(y, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIhtzd2gJQF2"
   },
   "source": [
    "## 4. Antrenarea rețelei neurale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cerinta 9.5] (2p) - Crearea si antrenarea unei arhitecturi de retea convolutionala similara cu LeNet\n",
    "\n",
    "Implementati [arhitectura LeNet](https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/) \n",
    "  - Conv(1, 28, 28, 20, 5, 1), Relu, MaxPool(2, 2), Conv(20, 12, 12, 50, 5, 1), Relu, MaxPool(2,2), Linearize(50, 4, 4), Linear(800, 500), Relu, Linear(500, 10), Softmax\n",
    "  \n",
    "**Adaptati codul de antrenare din laboratorul precedent pentru a antrena reteaua.**\n",
    "\n",
    "**Comparati rezultatele obtinute cu cele ale retelei MLP antrenate data trecuta.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 01 | Batch 468 | Train NLL:  0.281 | Train Acc:  95.83% | Test NLL:  0.176 | Test Acc: 94.46%\n",
      "Epoch 02 | Batch 468 | Train NLL:  0.180 | Train Acc:  98.96% | Test NLL:  0.116 | Test Acc: 96.44%\n",
      "Epoch 03 | Batch 468 | Train NLL:  0.184 | Train Acc:  98.96% | Test NLL:  0.106 | Test Acc: 96.94%\n",
      "Epoch 04 | Batch 468 | Train NLL:  0.183 | Train Acc:  98.96% | Test NLL:  0.123 | Test Acc: 96.30%\n",
      "Epoch 05 | Batch 468 | Train NLL:  0.176 | Train Acc:  98.96% | Test NLL:  0.112 | Test Acc: 96.61%\n",
      "Epoch 06 | Batch 468 | Train NLL:  0.198 | Train Acc:  98.96% | Test NLL:  0.110 | Test Acc: 96.66%\n",
      "Epoch 07 | Batch 468 | Train NLL:  0.183 | Train Acc:  98.96% | Test NLL:  0.109 | Test Acc: 96.62%\n",
      "Epoch 08 | Batch 468 | Train NLL:  0.166 | Train Acc:  98.96% | Test NLL:  0.114 | Test Acc: 96.45%\n",
      "Epoch 09 | Batch 468 | Train NLL:  0.175 | Train Acc:  98.96% | Test NLL:  0.100 | Test Acc: 97.03%\n",
      "Epoch 10 | Batch 468 | Train NLL:  0.160 | Train Acc:  98.96% | Test NLL:  0.084 | Test Acc: 97.45%\n",
      "Epoch 11 | Batch 468 | Train NLL:  0.148 | Train Acc:  98.96% | Test NLL:  0.079 | Test Acc: 97.60%\n",
      "Epoch 12 | Batch 468 | Train NLL:  0.157 | Train Acc:  98.96% | Test NLL:  0.084 | Test Acc: 97.52%\n",
      "Epoch 13 | Batch 468 | Train NLL:  0.168 | Train Acc:  98.96% | Test NLL:  0.087 | Test Acc: 97.46%\n",
      "Epoch 14 | Batch 468 | Train NLL:  0.177 | Train Acc:  98.96% | Test NLL:  0.095 | Test Acc: 97.21%\n",
      "Epoch 15 | Batch 468 | Train NLL:  0.163 | Train Acc:  98.96% | Test NLL:  0.096 | Test Acc: 97.37%\n",
      "Epoch 16 | Batch 468 | Train NLL:  0.150 | Train Acc:  98.96% | Test NLL:  0.089 | Test Acc: 97.61%\n",
      "Epoch 17 | Batch 468 | Train NLL:  0.141 | Train Acc:  98.96% | Test NLL:  0.093 | Test Acc: 97.48%\n",
      "Epoch 18 | Batch 468 | Train NLL:  0.140 | Train Acc:  98.96% | Test NLL:  0.099 | Test Acc: 97.29%\n",
      "Epoch 19 | Batch 468 | Train NLL:  0.140 | Train Acc:  98.96% | Test NLL:  0.099 | Test Acc: 97.40%\n",
      "Epoch 20 | Batch 468 | Train NLL:  0.144 | Train Acc:  98.96% | Test NLL:  0.090 | Test Acc: 97.56%\n"
     ]
    }
   ],
   "source": [
    "# TODO 9.5: antrenare retea LeNet\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_NO = 20\n",
    "\n",
    "optimize_args = {'mode': 'momentum', 'lr': .005}\n",
    "\n",
    "net = FeedForwardNetwork([\n",
    "    ConvolutionalLayer(1, 28, 28, 20, 5, 1),\n",
    "    ReLU(),\n",
    "    MaxPoolingLayer(2, 2),\n",
    "    ConvolutionalLayer(20, 12, 12, 50, 5, 1),\n",
    "    ReLU(),\n",
    "    MaxPoolingLayer(2, 2),\n",
    "    LinearizeLayer(50, 4, 4),\n",
    "    Linear(800, 500),\n",
    "    ReLU(),\n",
    "    Linear(500, 10)\n",
    "])\n",
    "cost_function = CrossEntropy()\n",
    "\n",
    "for epoch in range(EPOCHS_NO):\n",
    "    for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
    "        # 1. Prepare next batch\n",
    "        x = train_imgs[idx:idx + BATCH_SIZE,:,:]\n",
    "        t = train_labels[idx:idx + BATCH_SIZE]\n",
    "        x = np.expand_dims(x, axis=3)\n",
    "        x = x.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # 2. Compute gradient\n",
    "        y = net.forward(x, True)\n",
    "        loss = cost_function.forward(y, t)\n",
    "        dy = cost_function.backward(y, t)\n",
    "        dx = net.backward(dy)\n",
    "        \n",
    "        # 3. Update network parameters\n",
    "        net.update(**optimize_args)\n",
    "        \n",
    "        print(f'\\rEpoch {epoch + 1:02d} '\n",
    "              f'| Batch {b_no:03d} '\n",
    "              f'| Train NLL: {loss:6.3f} '\n",
    "              f'| Train Acc: {accuracy(y, t) * 100:6.2f}% ', end='')\n",
    "\n",
    "    t = np.expand_dims(test_imgs, axis=3)\n",
    "    y = net.forward(t.transpose(0, 3, 1, 2), train=False)\n",
    "    test_nll = cost_function.forward(y, test_labels)\n",
    "    print(f'| Test NLL: {test_nll:6.3f} '\n",
    "          f'| Test Acc: {accuracy(y, test_labels) * 100:3.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7zGgHlduJvA"
   },
   "source": [
    "## Teste\n",
    "\n",
    "Executați ```test0() and test16() and test7()``` pentru a rula testele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YaLsPBfuJvB"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm \n",
    "\n",
    "def close_enough(arr1, arr2, max_err = 0.0000001):\n",
    "    assert(arr1.shape == arr2.shape)\n",
    "    return norm(arr1.reshape(arr1.size) - arr2.reshape(arr2.size)) < max_err\n",
    "\n",
    "def test0():\n",
    "    fakex = [np.random.randn(128, n) for n in [20, 40, 30, 10]]\n",
    "\n",
    "    class DummyLayer:\n",
    "        def __init__(self, idx):\n",
    "            self.idx = idx\n",
    "\n",
    "        def forward(self, x):\n",
    "            return fakex[self.idx + 1]\n",
    "\n",
    "        def backward(self, x, dldy):\n",
    "            if not np.allclose(x, fakex[self.idx]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            if not np.allclose(dldy, -fakex[self.idx+1]):\n",
    "                raise Exception(\"Intrări greșite în backward\")\n",
    "            return -x\n",
    "\n",
    "    try:\n",
    "        net = FeedForwardNetwork([DummyLayer(i) for i in range(3)])\n",
    "        net.forward(fakex[0])\n",
    "        net.backward(-fakex[-1])\n",
    "        print(\"Cerința 0 rezolvată corect!\")\n",
    "        return True\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 0 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 0 are erori.\")\n",
    "        \n",
    "    return False\n",
    "        \n",
    "def test16():\n",
    "    __x = np.array([[-3.0731, -1.9081, -0.7283, -0.0757, -0.7577],\n",
    "                    [ 2.4041, -1.1506, -0.5924,  1.3016,  1.0882],\n",
    "                    [-0.5254,  0.3519, -0.9633, -2.7393, -0.9745]])\n",
    "    __w = np.array([[ 1.3214, -0.5886, -0.0351,  1.2084,  1.2661, -0.9979, -0.1172],\n",
    "                    [-0.4022,  0.1168,  0.9020, -2.0098, -0.5409, -0.3876, -0.1719],\n",
    "                    [-1.1125, -0.5556,  0.8843,  0.6995,  0.4929,  0.7523,  0.1832],\n",
    "                    [ 0.2267,  0.6757,  1.1286, -0.3218,  1.6934, -0.1782, -0.3467],\n",
    "                    [-0.6062,  0.4426,  0.5090,  0.4772, -0.5721,  0.8658, -0.5999]])\n",
    "    __b = np.array([ 0.3335,  0.5051, -0.1393,  1.2116,  1.7836, -0.6597,  0.3553])\n",
    "    __y = np.array([[-1.70746622, 2.10919555, -2.8676804, 0.48630531, -1.1288499, 1.95609904, 1.39083457],\n",
    "                    [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, -2.34822291, -0.94127596],\n",
    "                    [0.5391161, -0.89159687, -4.24288533, -0.38789499, -3.62798139, -1.35206921, 1.71422657]])\n",
    "    \n",
    "    __dy = np.array([[ 1.5555, -0.8978, -0.2917, -0.3868, -0.8257, -0.3491, -0.8658],\n",
    "                     [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0.4794,  0.8565],\n",
    "                     [-0.1552, -1.6319,  1.7642,  1.0503,  0.1035 , -0.7186, -0.9782]])\n",
    "    __dx = np.array([[ 1.53113221,  0.51455541, -2.588423,   -1.49460989, -0.98384103],\n",
    "                     [ 0.41215308,  0.46469672, -0.59552791,  3.04147235, -0.08763244],\n",
    "                     [ 2.92549149, -0.25707023,  2.70531668,  1.15769427,  0.67643021]])\n",
    "    __dw = np.array(\n",
    "        [[-2.01905511,  7.20190418,  2.2752849,   0.00865613,  3.89837344,  2.60289719, 5.23374791],\n",
    "         [-4.30512319, -0.57717827,  0.07387429,  1.40830543,  0.9345816,  -0.13835527, 0.3223155 ],\n",
    "         [-1.64365553,  1.34237165, -2.05517959, -0.57525343,  0.15290988,  0.66248035, 1.0654716 ],\n",
    "         [ 1.75815137,  6.47943337, -3.56222681, -3.18791411,  0.54523986,  2.61887489, 3.85994472],\n",
    "         [ 0.18554777,  3.89349109, -0.45449919, -1.01478565,  1.16539548,  1.48647185, 2.54131586]]\n",
    "    )\n",
    "    __db = np.array([ 2.5149, -1.0383,  2.4316,  0.4022, -0.1335, -0.5883, -0.9875])\n",
    "    \n",
    "    __y_relu = np.array([[0, 2.10919555, 0, 0.48630531, 0, 1.95609904, 1.39083457],\n",
    "                         [4.26749994, 0.64592254, 0.23749513, 6.11524068, 6.73936681, 0, 0],\n",
    "                         [0.5391161, 0, 0, 0, 0, 0, 1.71422657]])\n",
    "    __drelu = np.array([[0, -0.8978, 0, -0.3868, 0, -0.3491, -0.8658],\n",
    "                        [ 1.1146,  1.4914,  0.9591, -0.2613,  0.5887,  0,  0],\n",
    "                        [-0.1552, 0,  0,  0,  0 , 0, -0.9782]])\n",
    "    \n",
    "    __t = np.array([3, 1, 2])\n",
    "    __dl_dy = np.array(\n",
    "        [[ 2.80870645e-03,  1.27661957e-01,  8.80302096e-04, -3.08142112e-01,\n",
    "           5.00952130e-03,  1.09539948e-01,  6.22416775e-02],\n",
    "         [ 1.73238217e-02, -3.32870086e-01,  3.07917841e-04,  1.09927743e-01,\n",
    "           2.05192672e-01,  2.31991342e-05,  9.47329526e-05],\n",
    "         [ 6.60308812e-02,  1.57905168e-02, -3.32780047e-01,  2.61307149e-02,\n",
    "           1.02329216e-03,  9.96358772e-03,  2.13841054e-01]]\n",
    "    )\n",
    "\n",
    "\n",
    "    try:\n",
    "        lin = Linear(5, 7)\n",
    "        lin.weight = __w.copy()\n",
    "        lin.bias = __b.copy()\n",
    "        y = lin.forward(__x.copy())\n",
    "        if not np.allclose(y, __y):\n",
    "            raise Exception(\"Ieșiri greșite\")\n",
    "        print(\"Cerința 1 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 1 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 1 are erori.\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        dx = lin.backward(__x.copy(), __dy.copy())\n",
    "        if not np.allclose(dx, __dx):\n",
    "            raise ValueError(\"dL/dx greșit\")\n",
    "        if not np.allclose(lin.dweight, __dw):\n",
    "            raise ValueError(\"dL/dw greșit\")\n",
    "        if not np.allclose(lin.dbias, __db):\n",
    "            raise ValueError(\"dL/db greșit\")\n",
    "        print(\"Cerința 2 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 2 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 2 are erori.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        y_relu = relu.forward(__y.copy())\n",
    "        if not np.allclose(y_relu, __y_relu):\n",
    "            raise ValueError(\"ReLU(x) greșit\")\n",
    "        print(\"Cerința 3 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 3 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 3 are erori.\")\n",
    "        return False\n",
    "            \n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        drelu = relu.backward(__y.copy(), __dy.copy())\n",
    "        if not np.allclose(drelu, __drelu):\n",
    "            raise ValueError(\"ReLU.backward greșit\")\n",
    "        print(\"Cerința 4 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 4 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 4 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        loss = ce.forward(__y.copy(), __t.copy())\n",
    "        if np.abs(loss - 5.1874357237332545) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită nll: {loss:f} în loc de 5.1874357237332545\")\n",
    "        print(\"Cerința 5 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 5 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 5 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ce = CrossEntropy()\n",
    "        dl_dy = ce.backward(__y.copy(), __t.copy())\n",
    "        if not np.allclose(dl_dy, __dl_dy) > 1e-6:\n",
    "            raise ValueError(f\"Valoare greșită pentru dNLL/dy\")\n",
    "        print(\"Cerința 6 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 6 nu a fost implementată!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 6 are erori.\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def test7():  # Acuratețea\n",
    "    y = np.array([[ 0.6460014 , -0.05876393, -1.36496105, -0.07057596,  0.54938383],\n",
    "                  [-0.8033942 , -0.51753041,  0.92278036, -1.66303585, -0.36537512],\n",
    "                  [-1.3710599 ,  0.65598193, -0.75527154,  1.21609284,  0.08284123],\n",
    "                  [-1.24696857,  0.32676634,  0.09572539,  1.38316398, -0.14110726],\n",
    "                  [-2.01698315,  2.06123375, -1.68003675,  0.0504592 ,  0.04427597],\n",
    "                  [-0.8893451 ,  1.74695148, -0.29394473,  0.74203068, -0.75185261],\n",
    "                  [ 1.34126333, -0.5272606 ,  1.46458319,  1.59529987,  1.86884676],\n",
    "                  [-0.58987297,  1.10900165, -0.71208103,  0.20478154, -1.26693567],\n",
    "                  [-2.17730677, -1.36147532, -1.49679182,  0.24812177, -0.13368035],\n",
    "                  [-0.48730599,  1.31710647,  0.41765538,  1.19869192, -0.05301611],\n",
    "                  [-0.10655224, -0.21174034,  1.31548647, -0.57990281,  0.85868472],\n",
    "                  [-0.32055613, -2.17817118, -0.28488692,  1.62977524,  0.25150929],\n",
    "                  [ 0.07704727,  1.67710047,  1.83368441, -0.45456845, -0.74474969]])\n",
    "    t = np.array([0, 2, 3, 3, 1, 0, 1, 1, 2, 1, 2, 3, 2])\n",
    "    try:\n",
    "        acc = accuracy(y, t)\n",
    "        if np.abs(acc - 0.7692307692307693) > 1e-7:\n",
    "            raise ValueError(f\"{acc:f} != 10/13\")\n",
    "        print(f\"Cerința 7 rezolvată corect!\")\n",
    "    except NotImplementedError as e:\n",
    "        print(\"Cerința 7 nu a fost implementată!\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Cerința 7 are erori.\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_convolutional_layer():\n",
    "\n",
    "    np.random.seed(0)\n",
    "    l = ConvolutionalLayer(2, 3, 4, 3, 2, 1)\n",
    "\n",
    "    l.weights = np.random.rand(3, 2, 2, 2)\n",
    "\n",
    "    l.biases = np.random.rand(3, 1)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[[[ 34.55043437, 38.95942899, 43.36842361],\n",
    "                        [ 52.18641284, 56.59540746, 61.00440208]],\n",
    "    [[ 30.72457988, 34.08923073, 37.45388158],\n",
    "     [ 44.18318328, 47.54783413, 50.91248498]],\n",
    "     [[ 28.2244684, 31.30220961, 34.37995083],\n",
    "      [ 40.53543326, 43.61317448, 46.69091569]]]])\n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "    output_err = np.random.rand(1, 3, 2, 3)\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "    # print(l.g_biases)\n",
    "#    print(l.g_weights)\n",
    "#    print(g)\n",
    "\n",
    "    print(\"    i. testing gradients w.r.t. the bias terms...\")\n",
    "    gbias_target =  np.array([[ 2.4595299 ],\n",
    "                              [ 3.86207926],\n",
    "                              [ 1.17504241]])\n",
    "\n",
    "    print(l.g_biases.shape)\n",
    "    print(gbias_target.shape)\n",
    "\n",
    "    assert (l.g_biases.shape == gbias_target.shape), \"Wrong size\"\n",
    "    assert close_enough(l.g_biases, gbias_target), \"Wrong values\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"   ii. testing gradients w.r.t. the weights...\")\n",
    "    gweights_target = np.array(\n",
    "            [[[[ 12.19071134, 14.65024124],\n",
    "               [ 22.02883093, 24.48836083]],\n",
    "    [[ 41.70507011, 44.1646    ],\n",
    "     [ 51.54318969, 54.00271959]]],\n",
    "\n",
    "    [[[ 17.14269456, 21.00477382],\n",
    "      [ 32.59101161, 36.45309087]],\n",
    "\n",
    "  [[ 63.4876457 , 67.34972496],\n",
    "   [ 78.93596275, 82.79804201]]],\n",
    "\n",
    " [[[  5.38434096,  6.55938337],\n",
    "   [ 10.08451061, 11.25955302]],\n",
    "\n",
    "  [[ 19.4848499 , 20.65989231],\n",
    "   [ 24.18501955, 25.36006196]]]]\n",
    "    )\n",
    "\n",
    "    assert (l.g_weights.shape == gweights_target.shape), \"Wrong size\"\n",
    "    assert close_enough(l.g_weights, gweights_target), \"Wrong values\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_max_pooling_layer():\n",
    "\n",
    "    l = MaxPoolingLayer(2)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8]],\n",
    "                  [[9, 10, 11, 12], [13, 14, 15, 16]],\n",
    "                  [[17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[[[6, 8]],\n",
    "                       [[14, 16]],\n",
    "                       [[22, 24]]]])\n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "\n",
    "    output_err = output\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "    print(g)\n",
    "\n",
    "\n",
    "    print(\"Testing gradients\")\n",
    "    in_target = np.array([[[[0, 0, 0, 0], [0, 6, 0, 8]],\n",
    "                          [[0, 0, 0, 0], [0, 14, 0, 16]],\n",
    "                          [[0, 0, 0, 0], [0, 22, 0, 24]]]])\n",
    "\n",
    "    assert (g.shape == in_target.shape), \"Wrong size\"\n",
    "    assert close_enough(g, in_target), \"Wrong values in gradients\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"Backward computation implemented ok!\")\n",
    "    return True\n",
    "    \n",
    "\n",
    "def test_linearize_layer():\n",
    "\n",
    "    l = LinearizeLayer(2, 3, 4)\n",
    "\n",
    "    x = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n",
    "                  [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\n",
    "\n",
    "    print(\"Testing forward computation...\")\n",
    "    output = l.forward(x)\n",
    "    target = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24]])\n",
    "    target = target.reshape(1, len(target))\n",
    "    \n",
    "    assert (output.shape == target.shape), \"Wrong output size\"\n",
    "    assert close_enough(output, target), \"Wrong values in layer ouput\"\n",
    "    print(\"Forward computation implemented ok!\")\n",
    "\n",
    "    output_err = output\n",
    "\n",
    "    print(\"Testing backward computation...\")\n",
    "\n",
    "    g = l.backward(x, output_err)\n",
    "\n",
    "    print(\"Testing gradients\")\n",
    "    in_target = x\n",
    "\n",
    "    assert (g.shape == in_target.shape), \"Wrong size\"\n",
    "    assert close_enough(g, in_target), \"Wrong values in gradients\"\n",
    "    print(\"     OK\")\n",
    "\n",
    "    print(\"Backward computation implemented ok!\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWQ2e7C4uJvD"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cerința 0 rezolvată corect!\nCerința 1 rezolvată corect!\nCerința 2 rezolvată corect!\nCerința 3 rezolvată corect!\nCerința 4 rezolvată corect!\nCerința 5 rezolvată corect!\nCerința 6 rezolvată corect!\nCerința 7 rezolvată corect!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "test0() and test16() and test7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing forward computation...\nForward computation implemented ok!\nTesting backward computation...\n    i. testing gradients w.r.t. the bias terms...\n(3, 1)\n(3, 1)\n     OK\n   ii. testing gradients w.r.t. the weights...\n     OK\nTesting forward computation...\nForward computation implemented ok!\nTesting backward computation...\n[[[[ 0  0  0  0]\n   [ 0  6  0  8]]\n\n  [[ 0  0  0  0]\n   [ 0 14  0 16]]\n\n  [[ 0  0  0  0]\n   [ 0 22  0 24]]]]\nTesting gradients\n     OK\nBackward computation implemented ok!\nTesting forward computation...\nForward computation implemented ok!\nTesting backward computation...\nTesting gradients\n     OK\nBackward computation implemented ok!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test_convolutional_layer() and test_max_pooling_layer() and test_linearize_layer()"
   ]
  },
  {
   "source": [
    "## Concluzii\n",
    "Acuratetile retelei *LeNet* sunt:\n",
    "- fara momentum: 97.84%\n",
    "- cu momentum: 97.56%\n",
    "\n",
    "Pentru comparatie, acuratetile retelei *MLP* din laboratorul trecut sunt:\n",
    "- fara momentum: 95.61%\n",
    "- cu momentum: 97.81%\n",
    "\n",
    "Ceea ce se intampla, cel mai probabil, este ca setul de date este unul prea simplu pentru *LeNet*, drept care aceasta nu prea are ce sa mai invete dincolo de 98%. Se plafoneaza deja de pe la iteratia a 15-a, iar pana atunci, acuratetea de 97% se atinge din primele 10 epoci. Reteaua *MLP* e imbunatatita cu ~2% de adaugarea momentumului, dar si aceasta crestere este destul de mica. Cel mai probabil, diferentele reale dintre modele (*LeNet* ar trebui sa fie semnificativ mai bun) s-ar observa pe seturi de date si probleme mai complexe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Laborator 8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}