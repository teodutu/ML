{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "name": "Face-Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICtC53x1LOlB",
        "outputId": "31d3e55a-703b-4bef-bb8c-8d4e47d97985"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Running code @ {device}')\n",
        "\n",
        "!pip3 install facenet_pytorch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running code @ cuda\n",
            "Requirement already satisfied: facenet_pytorch in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet_pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrrVkRHjLOlG"
      },
      "source": [
        "def get_dataset():\n",
        "    return fetch_lfw_people(color=True)\n",
        "\n",
        "\n",
        "def get_small_dataset(og_ds, num_classes=5):\n",
        "    uniq, elem = np.unique(og_ds['target'], return_counts=True)\n",
        "    most_common = dict(sorted(zip(uniq, elem), key=lambda p: p[1], reverse=True)[:num_classes]).keys()\n",
        "\n",
        "    datas = []\n",
        "    images = []\n",
        "    targets = []\n",
        "    for i in og_ds['target']:\n",
        "        if i in most_common:\n",
        "            datas.append(og_ds['data'][i])\n",
        "            images.append(og_ds['images'][i])\n",
        "            targets.append(og_ds['target'][i])\n",
        "\n",
        "    return {\n",
        "        'data': np.array(datas),\n",
        "        'images': np.array(images),\n",
        "        'target': np.array(targets),\n",
        "        'target_names': np.array([og_ds['target_names'][tgt] for tgt in set(most_common)])\n",
        "    }\n",
        "\n",
        "# _ = get_og_dataset()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxNRYdBsSa4F"
      },
      "source": [
        "class FacesDataSet(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.trans = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "  \n",
        "        return self.trans(self.X[idx]), self.y[idx]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cXh1i4hLOlH",
        "outputId": "79836c6e-da47-4f09-ffe9-14bebdb1ba35"
      },
      "source": [
        "BATCH_SIZE_TRAIN = 100\n",
        "BATCH_SIZE_TEST = 100\n",
        "NO_WORKERS = 8\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "\n",
        "DS = get_dataset()\n",
        "SMALL_DS = get_small_dataset(DS)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(SMALL_DS['images'],\n",
        "    SMALL_DS['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "train_set = FacesDataSet(X_train, y_train)\n",
        "test_set = FacesDataSet(X_test, y_test)\n",
        "\n",
        "# print(f'Size of training images {X_train.shape}')\n",
        "# print(f'Size of training labels {y_train.shape}')\n",
        "# print(f'Size of test images {X_test.shape}')\n",
        "# print(f'Size of test labels {y_test.shape}')\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TRAIN, \n",
        "    shuffle=SHUFFLE_DATA, num_workers=NO_WORKERS)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, \n",
        "    shuffle=SHUFFLE_DATA, num_workers=NO_WORKERS)\n",
        "\n",
        "batch_train_images, batch_train_labels = next(iter(train_loader))\n",
        "# print(batch_train_images)\n",
        "# print(batch_train_labels)\n",
        "\n",
        "\n",
        "def loopy_test_loader(dl):\n",
        "    data_iter = iter(dl)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            yield next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dl)\n",
        "            yield next(data_iter)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exlnXfijLOlI",
        "outputId": "003c02fc-dd79-4146-f373-4b206e268deb"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, width, height, n_chan, n_cls):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_chan * width * height, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(64, n_cls)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "_, chan, w, h = batch_train_images.size()\n",
        "conv_net = ConvNet(w, h, chan, 5)\n",
        "conv_net = conv_net.to(device)\n",
        "\n",
        "print(list(conv_net.modules())[0])\n",
        "\n",
        "\n",
        "select = 2\n",
        "inputs = batch_train_images.to(device)[:select]\n",
        "target = batch_train_labels[:select]\n",
        "\n",
        "output = conv_net(inputs)\n",
        "_, predicted = torch.max(output, 1)\n",
        "\n",
        "print(output)\n",
        "print(predicted)\n",
        "print(target)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (conv3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout2): Dropout(p=0.25, inplace=False)\n",
            "  (dense1): Linear(in_features=231, out_features=231, bias=True)\n",
            "  (dropout3): Dropout(p=0.4, inplace=False)\n",
            "  (dense2): Linear(in_features=231, out_features=5, bias=True)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "tensor([[ 18.3619,   0.0000,  -8.7412,   0.0000,  -0.9017],\n",
            "        [  1.8342,   7.3835, -16.2854,   0.0000,   3.5989]], device='cuda:0',\n",
            "       grad_fn=<FusedDropoutBackward>)\n",
            "tensor([0, 1], device='cuda:0')\n",
            "tensor([1871, 1871])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MII0M0opLOlI",
        "outputId": "3f127ac7-3248-4cee-c1c6-52d41fc57340"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, width, height, n_chan, n_cls):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_chan, n_chan, kernel_size=5, stride=1)\n",
        "        self.conv2 = nn.Conv2d(n_chan, n_chan, kernel_size=5, stride=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(p=0.25)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(n_chan, n_chan, kernel_size=3, stride=1)\n",
        "        self.conv4 = nn.Conv2d(n_chan, n_chan, kernel_size=3, stride=1)\n",
        "        self.pool2 =  nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(p=0.25)\n",
        "\n",
        "        w_conv1 = (width - 5) + 1\n",
        "        h_conv1 = (height - 5) + 1\n",
        "        w_conv2 = (w_conv1 - 5) + 1\n",
        "        h_conv2 = (h_conv1 - 5) + 1\n",
        "        w_pool1 = (w_conv2 - 2) // 2 + 1\n",
        "        h_pool1 = (h_conv2 - 2) // 2 + 1\n",
        "\n",
        "        w_conv3 = (w_pool1 - 3) + 1\n",
        "        h_conv3 = (h_pool1 - 3) + 1\n",
        "        w_conv4 = (w_conv3 - 3) + 1\n",
        "        h_conv4 = (h_conv3 - 3) + 1\n",
        "        w_pool2 = (w_conv4 - 2) // 2 + 1\n",
        "        h_pool2 = (h_conv4 - 2) // 2 + 1\n",
        "\n",
        "        in_dense = n_chan * w_pool2 * h_pool2\n",
        "\n",
        "        self.dense1 = nn.Linear(in_dense, in_dense)\n",
        "        self.dropout3 = nn.Dropout(p=0.4)\n",
        "        self.dense2 = nn.Linear(in_dense, out_features=n_cls)\n",
        "        self.dropout4 = nn.Dropout(p=0.4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "_, chan, w, h = batch_train_images.size()\n",
        "conv_net = ConvNet(w, h, chan, 5)\n",
        "conv_net = conv_net.to(device)\n",
        "\n",
        "print(list(conv_net.modules())[0])\n",
        "\n",
        "\n",
        "select = 2\n",
        "inputs = batch_train_images.to(device)[:select]\n",
        "target = batch_train_labels[:select]\n",
        "\n",
        "output = conv_net(inputs)\n",
        "_, predicted = torch.max(output, 1)\n",
        "\n",
        "print(output)\n",
        "print(predicted)\n",
        "print(target)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout1): Dropout(p=0.25, inplace=False)\n",
            "  (conv3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout2): Dropout(p=0.25, inplace=False)\n",
            "  (dense1): Linear(in_features=231, out_features=231, bias=True)\n",
            "  (dropout3): Dropout(p=0.4, inplace=False)\n",
            "  (dense2): Linear(in_features=231, out_features=5, bias=True)\n",
            "  (dropout4): Dropout(p=0.4, inplace=False)\n",
            ")\n",
            "tensor([[ -6.7763,   0.0000, -32.9813, -11.6701,   0.0000],\n",
            "        [  0.0000,   3.7120,   0.0000,  -5.9164,   0.0000]], device='cuda:0',\n",
            "       grad_fn=<FusedDropoutBackward>)\n",
            "tensor([1, 1], device='cuda:0')\n",
            "tensor([1871, 1871])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJaElTQ6LOlK"
      },
      "source": [
        "\n",
        "def get_ds_facenet_vectors(ds, net, mtcnn):\n",
        "    return [net(mtcnn(img).unsqueeze(0)) for img in ds['images']]\n",
        "\n",
        "# vectors = get_ds_facenet_vectors(small_ds,\n",
        "#     InceptionResnetV1(pretrained='vggface2').eval(), MTCNN(margin=32))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auwOg57RLOlK"
      },
      "source": [
        "EPOCHS = 50\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "lr_init = 0.01\n",
        "lr_factor = 0.1\n",
        "weight_decay_factor = 1e-4\n",
        "lr_schedule_milestones = [90e3, 100e3, 110e3]\n",
        "\n",
        "\n",
        "def test_alg(net):\n",
        "    train_iter = 0\n",
        "    losses = []\n",
        "    steps = []\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=0.9, weight_decay=weight_decay_factor)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "    #optimizer = torch.optim.SGD(net.parameters(), lr=lr_init, momentum=0.9, weight_decay=weight_decay_factor)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01, weight_decay=weight_decay_factor)\n",
        "            \n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_schedule_milestones, gamma=lr_factor)\n",
        "\n",
        "    # simulate an inifinte test data provider by looping over the test data\n",
        "    test_data_provider = loopy_test_loader(test_loader)\n",
        "\n",
        "    # set model in train mode\n",
        "    net.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    ct = 0\n",
        "\n",
        "    for epoch in range(int(EPOCHS)):  # loop over the dataset multiple times\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # set the learning rate and decay according to iteration schedule\n",
        "            lr_scheduler.step()\n",
        "            \n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            running_acc += top_k_accuracy(1, labels, outputs)\n",
        "            \n",
        "            if train_iter % REPORT_TRAIN_EVERY == REPORT_TRAIN_EVERY - 1:    # print every REPORT_TRAIN_EVERY mini-batch iterations\n",
        "                train_loss = running_loss / REPORT_TRAIN_EVERY\n",
        "                train_acc = running_acc / REPORT_TRAIN_EVERY\n",
        "                \n",
        "                print('[%d, %5d, %6d] LR: %.5f' % (epoch + 1, i + 1, train_iter, lr_scheduler.get_lr()[-1]))\n",
        "                print('[%d, %5d] loss: %.5f, acc: %.5f' %\n",
        "                    (epoch + 1, i + 1, train_loss, train_acc))\n",
        "                \n",
        "                losses.append(train_loss)\n",
        "                steps.append(train_iter)\n",
        "                \n",
        "                running_loss = 0\n",
        "                train_loss = 0\n",
        "                running_acc = 0\n",
        "                train_acc = 0\n",
        "                \n",
        "                \n",
        "            if train_iter % PLOT_EVERY == 0:\n",
        "                plot_losses(losses, steps, train_iter)\n",
        "                \n",
        "            train_iter += 1\n",
        "        \n",
        "            if train_iter % REPORT_TEST_EVERY == 0:\n",
        "                # set model in test mode\n",
        "                net.eval()\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    # evaluate over at most TEST_ITER sub samples from the test_loader\n",
        "                    test_iter = 0\n",
        "                    test_loss = 0\n",
        "                    correct = 0\n",
        "                    \n",
        "                    while test_iter < TEST_ITERS:\n",
        "                    #for j, test_data in enumerate(test_loader, start=test_ct):\n",
        "                        test_data = next(test_data_provider)\n",
        "                            \n",
        "                        # get the test inputs; data is a list of [inputs, labels]\n",
        "                        test_inputs, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
        "                        \n",
        "                        out = net(test_inputs)\n",
        "                        test_loss += criterion(out, test_labels)\n",
        "                        \n",
        "                        correct += top_k_accuracy(1, test_labels, out)\n",
        "                        \n",
        "                        test_iter += 1\n",
        "                            \n",
        "                    avg_test_loss = test_loss / TEST_ITERS\n",
        "                    avg_acc = correct / TEST_ITERS\n",
        "                    \n",
        "                    print('[%d, %5d] avg_test_loss: %.5f, avg_test_acc: %.2f' \n",
        "                        % (epoch + 1, i + 1, avg_test_loss, avg_acc))\n",
        "                    \n",
        "                # set model back in train mode\n",
        "                net.train()\n",
        "        \n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "# test_alg(conv_net)"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}